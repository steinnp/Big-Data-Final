Single layer FFNN:
Validation score on 80/20 split
          0       0.66      0.64      0.65       233
          1       0.32      0.29      0.30       182
          2       0.72      0.76      0.74       425

avg / total       0.62      0.63      0.62       840
Training score was 99.3.


Next step: Add layers and add notes:
Adding layers and nodes did not increase the val score.
results :

             precision    recall  f1-score   support

          0       0.65      0.61      0.63       214
          1       0.29      0.29      0.29       161
          2       0.74      0.76      0.75       465

avg / total       0.63      0.63      0.63       840

That might mean that we are overfitting.

Next step: Add dropout.
First Added dropout to relatively simple network(one hidden layer, 16 nodes)
             precision    recall  f1-score   support

          0       0.68      0.73      0.71       208
          1       0.38      0.37      0.38       164
          2       0.79      0.78      0.78       468

avg / total       0.69      0.69      0.69       840

Looking good. Lets try a more complex model with dropout:

With this(bit more complex) model:
model = Sequential()
model.add(Dense(64, input_shape=(len(trainingReviews[0]), )))
model.add(Dropout(0.7))
model.add(Dense(32))
model.add(Dropout(0.7))
model.add(Dense(16))
model.add(Dropout(0.7))
model.add(Dense(len(trainingRatings[0]), activation='softmax'))

    model.compile(optimizer='rmsprop',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    we get:
                 precision    recall  f1-score   support

          0       0.72      0.73      0.72       220
          1       0.43      0.33      0.37       162
          2       0.80      0.87      0.84       458

avg / total       0.71      0.73      0.72       840

Getting better!



